{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "# for dataset in [X_train. X_test, y_train, y_test]:\n",
    "#     dataset = torch.Tensor(dataset)\n",
    "\n",
    "# get_tensor = lambda x: torch.from_numpy(x).float()\n",
    "X_train, X_test = list(map(torch.FloatTensor, [X_train, X_test]))\n",
    "y_train, y_test = list(map(torch.FloatTensor, [y_train, y_test]))\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([404])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    return ((y_pred - y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fqdg1\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([102])) that is different to the input size (torch.Size([404, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(22594036., grad_fn=<MseLossBackward>),\n",
       " tensor(1.6102e+09, grad_fn=<MseLossBackward>),\n",
       " tensor(1.4480e+11, grad_fn=<MseLossBackward>),\n",
       " tensor(1.3055e+13, grad_fn=<MseLossBackward>),\n",
       " tensor(1.1771e+15, grad_fn=<MseLossBackward>),\n",
       " tensor(1.0613e+17, grad_fn=<MseLossBackward>),\n",
       " tensor(9.5688e+18, grad_fn=<MseLossBackward>),\n",
       " tensor(8.6275e+20, grad_fn=<MseLossBackward>),\n",
       " tensor(7.7788e+22, grad_fn=<MseLossBackward>),\n",
       " tensor(7.0135e+24, grad_fn=<MseLossBackward>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(X_train.shape[1], 1)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')  # F.mse_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "\n",
    "x_norm = norm(X_train)\n",
    "epochs_cache = []\n",
    "losses_cache = []\n",
    "import pdb\n",
    "for epoch in range(20):\n",
    "#     inputs = Variable(x_norm)\n",
    "#     labels = Variable(x_norm)\n",
    "#     model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(x_norm)\n",
    "    \n",
    "    # Compute Loss\n",
    "    current_loss = criterion(y_pred, y_test) #loss(y_pred, y_train) \n",
    "#     if epoch < 10 == 0:\n",
    "#     print(\"current_loss =\", current_loss)\n",
    "#     pdb.set_trace()\n",
    "    \n",
    "    # Backward pass\n",
    "    current_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    epochs_cache.append(epoch)\n",
    "    losses_cache.append(current_loss)\n",
    "    \n",
    "losses_cache[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # %debug\n",
    "\n",
    "# # torch.Tensor?\n",
    "\n",
    "# M = torch.Tensor([[1, 2, 3], [3, 2, 4]])\n",
    "# # mat1 = torch.randn(2, 3)\n",
    "# # mat2 = torch.randn(3, 3)\n",
    "# # torch.addmm(M, mat1, mat2)\n",
    "# M\n",
    "\n",
    "# new_x = torch.from_numpy(X_test)\n",
    "# y_pred = model(new_x)\n",
    "# print(\"predicted Y value: \", y_pred.data[0][0])\n",
    "\n",
    "# x = Variable(torch.Tensor(X_train))\n",
    "# y = model(x, x.shape, x.shape)\n",
    "# y\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# torch.from_numpy(np.array([1, 2])), Variable([np.array([1])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
