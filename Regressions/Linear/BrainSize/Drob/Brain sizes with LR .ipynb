{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c41613",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- # Data in the notebook got from anthropologist [Stanislav Drobyshevsky](https://vk.com/id15342645) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b337a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import init_param, model  # , compute_cost, init_adam, adam_optimizer, forward_prop, backward_prop\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"headbrain.csv\")\n",
    "X = data.iloc[:, 0].to_numpy().reshape(-1, 1)\n",
    "y = data.iloc[:, 1].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b44e36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    predict = X.dot(theta)\n",
    "    cost = 1/(2*m) * np.sum(np.square(predict - y))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, lr=0.01, iters=10):\n",
    "    m = X.shape[0]\n",
    "    cost_history = np.zeros(iters)\n",
    "    theta_history = np.zeros((iters, 2))\n",
    "    for i in range(iters):\n",
    "        predict = np.dot(X, theta)\n",
    "        theta = theta - (1/m) * lr * (X.T.dot((predict - y)))  # 1/m * lr * X.T.dot(predict - y)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        theta_history[i, :] = theta.T\n",
    "        cost_history[i] = compute_cost(theta, X, y)\n",
    "#         print(cost_history[i])\n",
    "    return theta, cost_history, theta_history\n",
    "        \n",
    "# def gradient_descent(X, y, theta, lr=0.01, iters=10):\n",
    "#     m = X.shape[0]\n",
    "#     cost_history = np.zeros(iters)\n",
    "#     theta_history = np.zeros((iters, 2))\n",
    "#     for i in range(iters):\n",
    "#         predict = X.dot(theta)\n",
    "#         theta -= (1/m) * lr * (X.T.dot((predict - y)))  # 1/m * lr * X.T.dot(predict - y)\n",
    "# #         import pdb; pdb.set_trace()\n",
    "#         theta_history[i, :] = theta.T\n",
    "#         cost_history[i] = compute_cost(theta, X, y)\n",
    "\n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "\n",
    "# X = np.concatenate((np.ones((m, 1)), X.to_numpy().reshape(-1, 1)), axis=1)\n",
    "# y = y.to_numpy().reshape((-1, 1))\n",
    "theta = np.random.randn(X.shape[1], 1)  # init_param(*X.shape)\n",
    "iters = 1000\n",
    "# X_b = np.c_(np.ones((X.s)))\n",
    "theta, cost_history, theta_history = gradient_descent(X, y, theta, iters=iters)\n",
    "\n",
    "plt.plot(range(iters), cost_history)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}